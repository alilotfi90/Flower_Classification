{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLQ8bQJCHoUQBACuDa7s19",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alilotfi90/Flower_Classification/blob/main/grayscale_vs_colour_experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqHwFruCVzdW",
        "outputId": "d2bdbb78-1a5a-47e7-e277-dc7b520ef7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from zipfile import ZipFile\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/flowerimages.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('unzipped_images_flower')"
      ],
      "metadata": {
        "id": "sQhANjxSWXYZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "unzipped_folder = '/content/unzipped_images_flower/flowers'\n",
        "output_folder = '/content/resized_images_flower'\n",
        "\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "subdirectories = [d for d in os.listdir(unzipped_folder) if os.path.isdir(os.path.join(unzipped_folder, d))]\n",
        "\n",
        "# seen = set()\n",
        "for flower_category in subdirectories:\n",
        "    input_folder = os.path.join(unzipped_folder, flower_category)\n",
        "    output_category_folder = os.path.join(output_folder, flower_category)\n",
        "\n",
        "\n",
        "    os.makedirs(output_category_folder, exist_ok=True)\n",
        "\n",
        "    image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "      input_image_path = os.path.join(input_folder, image_file)\n",
        "      output_image_path = os.path.join(output_category_folder, image_file)\n",
        "\n",
        "      image = Image.open(input_image_path)\n",
        "\n",
        "      # Ensure the image is RGB\n",
        "      image = image.convert(\"RGB\")\n",
        "\n",
        "      # Resize the image\n",
        "      target_size = (224, 224)\n",
        "      image = image.resize(target_size, Image.ANTIALIAS)\n",
        "\n",
        "      image.save(output_image_path)\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "max_images_per_class =40\n",
        "data = []\n",
        "\n",
        "for flower_category in subdirectories:\n",
        "    category_folder = os.path.join(output_folder, flower_category)\n",
        "    image_files = [f for f in os.listdir(category_folder) if f.endswith('.jpg')]\n",
        "    label = subdirectories.index(flower_category)  # label = folder's name\n",
        "\n",
        "    # Limit the number of images to max_images_per_class\n",
        "    image_files = image_files[:max_images_per_class]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(category_folder, image_file)\n",
        "        data.append((image_path, label))\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL-wechJWcuh",
        "outputId": "b9a02a10-64f2-4b9c-a8a5-4b4e1097410b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-251caeb44185>:33: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image = image.resize(target_size, Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.grayscale_count = 0  # Adding a counter for grayscale images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # we don't really need the following, we changed rare grayscale images to rgb when feeding to the colour_model\n",
        "        # Check and convert grayscale images\n",
        "        # if image.mode != 'RGB':\n",
        "        #     image = image.convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "transform_gray = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "transform_color = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Data Loaders\n",
        "train_loader_gray = DataLoader(CustomDataset(train_data, transform=transform_gray), batch_size=32, shuffle=True)\n",
        "test_loader_gray = DataLoader(CustomDataset(test_data, transform=transform_gray), batch_size=32)\n",
        "train_loader_color = DataLoader(CustomDataset(train_data, transform=transform_color), batch_size=32, shuffle=True)\n",
        "test_loader_color = DataLoader(CustomDataset(test_data, transform=transform_color), batch_size=32)\n",
        "\n",
        "class CustomEfficientNet(nn.Module):\n",
        "    def __init__(self, pretrained_model_name='efficientnet-b0', num_classes=16):\n",
        "        super(CustomEfficientNet, self).__init__()\n",
        "        self.embedding = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
        "        self.eff_net = EfficientNet.from_pretrained(pretrained_model_name, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.eff_net(x)\n",
        "        return x\n",
        "\n",
        "# Model Instances\n",
        "model_gray = CustomEfficientNet().to(device)\n",
        "model_color = EfficientNet.from_pretrained('efficientnet-b0', num_classes=16).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_gray = optim.SGD(model_gray.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_color = optim.SGD(model_color.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_test_model(model, train_loader, test_loader, optimizer, criterion, epochs_list, model_name):\n",
        "    cumulative_epochs = 0\n",
        "    for epochs in epochs_list:\n",
        "        cumulative_epochs += epochs\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                if i % 10 == 9:\n",
        "                    print(f'[{cumulative_epochs + epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}')\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of {model_name} after {cumulative_epochs} epochs on test images: {accuracy:.2f}%')\n",
        "    return accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHqrTeY2XfLM",
        "outputId": "13f42e69-71e7-41ab-e3de-a66d6ecfb81f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_list = [10, 20, 30,40]\n",
        "# Assuming `accuracies_gray` is obtained similarly...\n",
        "accuracies_gray = train_test_model(\n",
        "    model=model_gray,\n",
        "    train_loader=train_loader_gray,\n",
        "    test_loader=test_loader_gray,\n",
        "    optimizer=optimizer_gray,\n",
        "    criterion=criterion,\n",
        "    epochs_list=epochs_list,\n",
        "    model_name=\"Grayscale Model\"\n",
        ")\n",
        "accuracies_color = train_test_model(\n",
        "    model=model_color,\n",
        "    train_loader=train_loader_color,\n",
        "    test_loader=test_loader_color,\n",
        "    optimizer=optimizer_color,\n",
        "    criterion=criterion,\n",
        "    epochs_list=epochs_list,\n",
        "    model_name=\"Color Model\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Creating DataFrame and Save to Google Drive\n",
        "df = pd.DataFrame({\n",
        "    'Epochs': [sum(epochs_list[:i+1]) for i in range(len(epochs_list))],\n",
        "    'Color_Model_Accuracy': accuracies_color,\n",
        "    'Grayscale_Model_Accuracy': accuracies_gray,\n",
        "})\n",
        "\n",
        "# Save to Google Drive\n",
        "# Ensure you've mounted your Google Drive if using Colab.\n",
        "df.to_csv('/content/drive/MyDrive/model_accuracies.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbRVH7T0vn1P",
        "outputId": "078b8766-eb29-460c-cbde-3e7505dbc74d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 10] loss: 2.795\n",
            "[12, 10] loss: 2.741\n",
            "[13, 10] loss: 2.609\n",
            "[14, 10] loss: 2.529\n",
            "[15, 10] loss: 2.420\n",
            "[16, 10] loss: 2.308\n",
            "[17, 10] loss: 2.212\n",
            "[18, 10] loss: 2.096\n",
            "[19, 10] loss: 2.037\n",
            "[20, 10] loss: 1.906\n",
            "Accuracy of Grayscale Model after 10 epochs on test images: 48.44%\n",
            "[31, 10] loss: 1.833\n",
            "[32, 10] loss: 1.684\n",
            "[33, 10] loss: 1.621\n",
            "[34, 10] loss: 1.531\n",
            "[35, 10] loss: 1.447\n",
            "[36, 10] loss: 1.310\n",
            "[37, 10] loss: 1.245\n",
            "[38, 10] loss: 1.192\n",
            "[39, 10] loss: 1.144\n",
            "[40, 10] loss: 1.082\n",
            "[41, 10] loss: 0.971\n",
            "[42, 10] loss: 1.007\n",
            "[43, 10] loss: 0.914\n",
            "[44, 10] loss: 0.823\n",
            "[45, 10] loss: 0.763\n",
            "[46, 10] loss: 0.766\n",
            "[47, 10] loss: 0.707\n",
            "[48, 10] loss: 0.656\n",
            "[49, 10] loss: 0.598\n",
            "[50, 10] loss: 0.562\n",
            "Accuracy of Grayscale Model after 30 epochs on test images: 77.34%\n",
            "[61, 10] loss: 0.571\n",
            "[62, 10] loss: 0.511\n",
            "[63, 10] loss: 0.533\n",
            "[64, 10] loss: 0.511\n",
            "[65, 10] loss: 0.474\n",
            "[66, 10] loss: 0.459\n",
            "[67, 10] loss: 0.401\n",
            "[68, 10] loss: 0.385\n",
            "[69, 10] loss: 0.390\n",
            "[70, 10] loss: 0.378\n",
            "[71, 10] loss: 0.301\n",
            "[72, 10] loss: 0.315\n",
            "[73, 10] loss: 0.294\n",
            "[74, 10] loss: 0.279\n",
            "[75, 10] loss: 0.284\n",
            "[76, 10] loss: 0.278\n",
            "[77, 10] loss: 0.252\n",
            "[78, 10] loss: 0.262\n",
            "[79, 10] loss: 0.221\n",
            "[80, 10] loss: 0.230\n",
            "[81, 10] loss: 0.206\n",
            "[82, 10] loss: 0.191\n",
            "[83, 10] loss: 0.172\n",
            "[84, 10] loss: 0.179\n",
            "[85, 10] loss: 0.165\n",
            "[86, 10] loss: 0.198\n",
            "[87, 10] loss: 0.174\n",
            "[88, 10] loss: 0.178\n",
            "[89, 10] loss: 0.169\n",
            "[90, 10] loss: 0.162\n",
            "Accuracy of Grayscale Model after 60 epochs on test images: 78.91%\n",
            "[101, 10] loss: 0.153\n",
            "[102, 10] loss: 0.129\n",
            "[103, 10] loss: 0.132\n",
            "[104, 10] loss: 0.133\n",
            "[105, 10] loss: 0.116\n",
            "[106, 10] loss: 0.119\n",
            "[107, 10] loss: 0.100\n",
            "[108, 10] loss: 0.098\n",
            "[109, 10] loss: 0.124\n",
            "[110, 10] loss: 0.096\n",
            "[111, 10] loss: 0.094\n",
            "[112, 10] loss: 0.094\n",
            "[113, 10] loss: 0.089\n",
            "[114, 10] loss: 0.089\n",
            "[115, 10] loss: 0.096\n",
            "[116, 10] loss: 0.082\n",
            "[117, 10] loss: 0.100\n",
            "[118, 10] loss: 0.095\n",
            "[119, 10] loss: 0.074\n",
            "[120, 10] loss: 0.082\n",
            "[121, 10] loss: 0.067\n",
            "[122, 10] loss: 0.065\n",
            "[123, 10] loss: 0.065\n",
            "[124, 10] loss: 0.073\n",
            "[125, 10] loss: 0.064\n",
            "[126, 10] loss: 0.074\n",
            "[127, 10] loss: 0.066\n",
            "[128, 10] loss: 0.066\n",
            "[129, 10] loss: 0.073\n",
            "[130, 10] loss: 0.059\n",
            "[131, 10] loss: 0.055\n",
            "[132, 10] loss: 0.054\n",
            "[133, 10] loss: 0.060\n",
            "[134, 10] loss: 0.068\n",
            "[135, 10] loss: 0.052\n",
            "[136, 10] loss: 0.052\n",
            "[137, 10] loss: 0.053\n",
            "[138, 10] loss: 0.071\n",
            "[139, 10] loss: 0.055\n",
            "[140, 10] loss: 0.044\n",
            "Accuracy of Grayscale Model after 100 epochs on test images: 78.12%\n",
            "[11, 10] loss: 2.803\n",
            "[12, 10] loss: 2.692\n",
            "[13, 10] loss: 2.559\n",
            "[14, 10] loss: 2.417\n",
            "[15, 10] loss: 2.278\n",
            "[16, 10] loss: 2.159\n",
            "[17, 10] loss: 2.012\n",
            "[18, 10] loss: 1.867\n",
            "[19, 10] loss: 1.755\n",
            "[20, 10] loss: 1.604\n",
            "Accuracy of Color Model after 10 epochs on test images: 69.53%\n",
            "[31, 10] loss: 1.495\n",
            "[32, 10] loss: 1.374\n",
            "[33, 10] loss: 1.281\n",
            "[34, 10] loss: 1.171\n",
            "[35, 10] loss: 1.048\n",
            "[36, 10] loss: 0.994\n",
            "[37, 10] loss: 0.886\n",
            "[38, 10] loss: 0.816\n",
            "[39, 10] loss: 0.769\n",
            "[40, 10] loss: 0.759\n",
            "[41, 10] loss: 0.674\n",
            "[42, 10] loss: 0.609\n",
            "[43, 10] loss: 0.555\n",
            "[44, 10] loss: 0.558\n",
            "[45, 10] loss: 0.527\n",
            "[46, 10] loss: 0.462\n",
            "[47, 10] loss: 0.474\n",
            "[48, 10] loss: 0.419\n",
            "[49, 10] loss: 0.395\n",
            "[50, 10] loss: 0.385\n",
            "Accuracy of Color Model after 30 epochs on test images: 85.94%\n",
            "[61, 10] loss: 0.324\n",
            "[62, 10] loss: 0.353\n",
            "[63, 10] loss: 0.334\n",
            "[64, 10] loss: 0.291\n",
            "[65, 10] loss: 0.315\n",
            "[66, 10] loss: 0.254\n",
            "[67, 10] loss: 0.238\n",
            "[68, 10] loss: 0.228\n",
            "[69, 10] loss: 0.221\n",
            "[70, 10] loss: 0.200\n",
            "[71, 10] loss: 0.200\n",
            "[72, 10] loss: 0.198\n",
            "[73, 10] loss: 0.194\n",
            "[74, 10] loss: 0.161\n",
            "[75, 10] loss: 0.169\n",
            "[76, 10] loss: 0.164\n",
            "[77, 10] loss: 0.166\n",
            "[78, 10] loss: 0.169\n",
            "[79, 10] loss: 0.135\n",
            "[80, 10] loss: 0.149\n",
            "[81, 10] loss: 0.140\n",
            "[82, 10] loss: 0.137\n",
            "[83, 10] loss: 0.123\n",
            "[84, 10] loss: 0.112\n",
            "[85, 10] loss: 0.126\n",
            "[86, 10] loss: 0.119\n",
            "[87, 10] loss: 0.113\n",
            "[88, 10] loss: 0.102\n",
            "[89, 10] loss: 0.085\n",
            "[90, 10] loss: 0.097\n",
            "Accuracy of Color Model after 60 epochs on test images: 86.72%\n",
            "[101, 10] loss: 0.082\n",
            "[102, 10] loss: 0.085\n",
            "[103, 10] loss: 0.094\n",
            "[104, 10] loss: 0.078\n",
            "[105, 10] loss: 0.094\n",
            "[106, 10] loss: 0.074\n",
            "[107, 10] loss: 0.069\n",
            "[108, 10] loss: 0.069\n",
            "[109, 10] loss: 0.076\n",
            "[110, 10] loss: 0.070\n",
            "[111, 10] loss: 0.072\n",
            "[112, 10] loss: 0.067\n",
            "[113, 10] loss: 0.067\n",
            "[114, 10] loss: 0.066\n",
            "[115, 10] loss: 0.065\n",
            "[116, 10] loss: 0.063\n",
            "[117, 10] loss: 0.061\n",
            "[118, 10] loss: 0.061\n",
            "[119, 10] loss: 0.067\n",
            "[120, 10] loss: 0.057\n",
            "[121, 10] loss: 0.058\n",
            "[122, 10] loss: 0.058\n",
            "[123, 10] loss: 0.059\n",
            "[124, 10] loss: 0.051\n",
            "[125, 10] loss: 0.060\n",
            "[126, 10] loss: 0.052\n",
            "[127, 10] loss: 0.037\n",
            "[128, 10] loss: 0.058\n",
            "[129, 10] loss: 0.038\n",
            "[130, 10] loss: 0.043\n",
            "[131, 10] loss: 0.036\n",
            "[132, 10] loss: 0.041\n",
            "[133, 10] loss: 0.034\n",
            "[134, 10] loss: 0.041\n",
            "[135, 10] loss: 0.047\n",
            "[136, 10] loss: 0.045\n",
            "[137, 10] loss: 0.048\n",
            "[138, 10] loss: 0.036\n",
            "[139, 10] loss: 0.035\n",
            "[140, 10] loss: 0.035\n",
            "Accuracy of Color Model after 100 epochs on test images: 87.50%\n"
          ]
        }
      ]
    }
  ]
}