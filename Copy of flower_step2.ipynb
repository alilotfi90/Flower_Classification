{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37562,"status":"ok","timestamp":1697170580651,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"c3uls5YVD_HF","outputId":"09daa676-46d2-49b9-aa12-12c9374537e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-\u003eefficientnet_pytorch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch-\u003eefficientnet_pytorch) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch-\u003eefficientnet_pytorch) (17.0.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch-\u003eefficientnet_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch-\u003eefficientnet_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet_pytorch\n","  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=d75d68ed9474a50bd6f6aec7effbb7fd062a1516e7ccea9bd500f7096f3d83ca\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","Successfully built efficientnet_pytorch\n","Installing collected packages: efficientnet_pytorch\n","Successfully installed efficientnet_pytorch-0.7.1\n","Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","\n","!pip install efficientnet_pytorch\n","from zipfile import ZipFile\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from efficientnet_pytorch import EfficientNet\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13066,"status":"ok","timestamp":1697170596483,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"FU_rM-0sEU_T"},"outputs":[],"source":["import zipfile\n","\n","\n","with zipfile.ZipFile('/content/drive/MyDrive/flowerimages.zip', 'r') as zip_ref:\n","    zip_ref.extractall('unzipped_images_flower')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9209,"status":"ok","timestamp":1697170606914,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"-QmvcmhFFRUd","outputId":"3bd27768-9ebd-4a1d-97ad-224113b285ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"]}],"source":["!pip install Pillow\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4586,"status":"ok","timestamp":1697170621569,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"RP-8ygduFYSU","outputId":"65ec2cfd-a350-4b08-cd17-b8024fc25752"},"outputs":[{"name":"stdout","output_type":"stream","text":["272\n"]}],"source":["from PIL import Image\n","import os\n","\n","\n","unzipped_folder = '/content/unzipped_images_flower/flowers'\n","\n","\n","subdirectories = [d for d in os.listdir(unzipped_folder) if os.path.isdir(os.path.join(unzipped_folder, d))]\n","\n","seen = set()\n","for flower_category in subdirectories:\n","    folder_path = os.path.join(unzipped_folder, flower_category)\n","    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n","\n","    for image_file in image_files:\n","        image_path = os.path.join(folder_path, image_file)\n","        image = Image.open(image_path)\n","        width, height = image.size\n","        seen.add((width, height))\n","\n","# lets check if all images have the same dimensions\n","print(len(seen))\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135162,"status":"ok","timestamp":1697170762078,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"44ii6FDBTo5l","outputId":"93757f18-4ba1-46f1-e0e2-4cb30fb91ab9"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-5-9eb77c6bfed0\u003e:30: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  image = image.resize(target_size, Image.ANTIALIAS)\n"]}],"source":["from PIL import Image\n","import os\n","\n","unzipped_folder = '/content/unzipped_images_flower/flowers'\n","output_folder = '/content/resized_images_flower'\n","\n","\n","os.makedirs(output_folder, exist_ok=True)\n","\n","subdirectories = [d for d in os.listdir(unzipped_folder) if os.path.isdir(os.path.join(unzipped_folder, d))]\n","\n","seen = set()\n","for flower_category in subdirectories:\n","    input_folder = os.path.join(unzipped_folder, flower_category)\n","    output_category_folder = os.path.join(output_folder, flower_category)\n","\n","\n","    os.makedirs(output_category_folder, exist_ok=True)\n","\n","    image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n","\n","    for image_file in image_files:\n","        input_image_path = os.path.join(input_folder, image_file)\n","        output_image_path = os.path.join(output_category_folder, image_file)\n","\n","        image = Image.open(input_image_path)\n","\n","        # Resize the image\n","        target_size = (456, 456)\n","        image = image.resize(target_size, Image.ANTIALIAS)\n","\n","        # Convert the image to grayscale\n","        # image = image.convert('L')\n","\n","\n","        image.save(output_image_path)\n","\n","        width, height = image.size\n","        seen.add((width, height))\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1697170934149,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"FzzC4crTXm5Z"},"outputs":[],"source":["# from sklearn.model_selection import train_test_split\n","\n","\n","# data = []\n","\n","# for flower_category in subdirectories:\n","#     category_folder = os.path.join(output_folder, flower_category)\n","#     image_files = [f for f in os.listdir(category_folder) if f.endswith('.jpg')]\n","#     label = subdirectories.index(flower_category)  # Assign a label based on the folder name\n","\n","#     for image_file in image_files:\n","#         image_path = os.path.join(category_folder, image_file)\n","#         data.append((image_path, label))\n","\n","\n","# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","max_images_per_class =40\n","\n","data = []\n","\n","for flower_category in subdirectories:\n","    category_folder = os.path.join(output_folder, flower_category)\n","    image_files = [f for f in os.listdir(category_folder) if f.endswith('.jpg')]\n","    label = subdirectories.index(flower_category)  # Assign a label based on the folder name\n","\n","    # Limit the number of images to max_images_per_class\n","    image_files = image_files[:max_images_per_class]\n","\n","    for image_file in image_files:\n","        image_path = os.path.join(category_folder, image_file)\n","        data.append((image_path, label))\n","\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1697170950958,"user":{"displayName":"Ali Lotfi","userId":"05228479347581948117"},"user_tz":360},"id":"h1xSdAaaXurv"},"outputs":[],"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image_path, label = self.data[idx]\n","        image = Image.open(image_path)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","train_dataset = CustomDataset(train_data, transform=transform)\n","test_dataset = CustomDataset(test_data, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CfgngrXEXwuh"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","100%|██████████| 20.4M/20.4M [00:00\u003c00:00, 287MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b0\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","for number of epochs being: 10\n","Accuracy of the network on the test images: 67 %\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","for number of epochs being: 20\n","Accuracy of the network on the test images: 79 %\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","for number of epochs being: 30\n","Accuracy of the network on the test images: 83 %\n"]}],"source":["\n","model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=16)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","max_epoch = [10, 20, 30]\n","for e in max_epoch:\n","    # Training\n","    for epoch in range(e):\n","        print(epoch)\n","        running_loss = 0.0\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            if i % 50 == 49:\n","                print('[%d, %5d] loss: %.3f' %\n","                      (epoch + 1, i + 1, running_loss / 100))\n","                running_loss = 0.0\n","\n","    # Testing\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in test_loader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(\"for number of epochs being:\", e)\n","    print('Accuracy of the network on the test images: %d %%' % (\n","        100 * correct / total))\n","\n","# Save the trained model\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/images/flower_efficient30\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNP3Of/g5A+Tm3jqQORJyuw","name":"","provenance":[{"file_id":"1pocBiqB7TL-7ERxoANf9u9CL3vhW5_wH","timestamp":1697156352054}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}